{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b244b7db",
   "metadata": {},
   "source": [
    "## Save_and_load_the_model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c20bb0",
   "metadata": {},
   "source": [
    "모델을 학습시키고 어느정도 좋은 성능을 낸다면 이 모델을 저장하여야 한다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2527baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx as onnx\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cf2fd8",
   "metadata": {},
   "source": [
    "사전학습된 모델을 불러온 후, `.state_dict()`메소드를 사용하여 모델의 파라미터를 저장한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3575e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d6645",
   "metadata": {},
   "source": [
    "저장한 모델을 불러올 때에는 이전에 모델의 파라미터만을 저장했기 때문에 이 파라미터들을 저장할 모델의 구조가 필요하다. 따라서 사전학습되지 않은 모델을 불러온 후, 이 모델의 파라미터에 저장한 사전학습된 모델의 파라미터를 덮어쓴다. \n",
    "이때 `load_state_dict()`와 `torch.load()`를 사용하여 불러올 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31f73ee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.vgg16() # 기본 가중치를 불러오지 않으므로 pretrained=True를 지정하지 않습니다.\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d779c011",
   "metadata": {},
   "source": [
    "마지막줄의 model.eval()은 모델을 inference용으로 사용하기 위해 설정해 주는 것이다. \n",
    "예를 들어 드롭아웃의 경우 학습시와 테스트시의 작동방식이 다르기 때문에 eval()을 사용해서 모델을 추론용으로 변환해 주어야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635cf9b1",
   "metadata": {},
   "source": [
    "위에서 모델의 파라미터만을 저장하는 방식을 알아보았는데 모델의 구조와 함께 모델 자체를 저장할 수 있다. \n",
    "이때 `.save()`메소드를 사용하여 모델을 저장할 수 있으며 불러올 때에도 `load()`메소드를 사용하여 사용할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1dc51ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78c2b67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9476eb",
   "metadata": {},
   "source": [
    "ONNX의 형식으로도 모델을 저장할 수 있다. ONNX라 하면 간단히 말해서 다른 프레임워크에서도 사용할 수 있는 형태로 모델을 저장하는 것이다. \n",
    "`onnx.export()`메소드를 사용하여 모델을 ONNX의 형태로 저장할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ac33242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.zeros((1,3,224,224))\n",
    "onnx.export(model, input_image, 'model.onnx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
